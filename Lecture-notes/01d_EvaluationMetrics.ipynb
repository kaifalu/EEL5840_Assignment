{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Error and Evaluation Metrics\n",
    "\n",
    "* A key step in machine learning algorithm development and testing is determining a good error and evaluation metric. \n",
    "\n",
    "* Evaluation metrics help us to estimate how well our model is trained and it is important to pick a metric that matches our overall goal for the system.  \n",
    "\n",
    "* Some common evaluation metrics include precision, recall, receiver operating curves, and confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification Accuracy and Error \n",
    "\n",
    "* Classification accuracy is defined as the number of correctly classified samples divided by all samples:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{accuracy} = \\frac{N_{cor}}{N} \n",
    "\\end{equation}\n",
    "where $N_{cor}$ is the number of correct classified samples and $N$ is the total number of samples.\n",
    "\n",
    "* Classification error is defined as the number of incorrectly classified samples divided by all samples:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{error} = \\frac{N_{mis}}{N}\n",
    "\\end{equation}\n",
    "where $N_{mis}$ is the number of misclassified samples and $N$ is the total number of samples.\n",
    "\n",
    "* Suppose there is a 3-class classification problem, in which we would like to classify each training sample (a fish) to one of the three classes (A = salmon or B = sea bass or C = cod). \n",
    "\n",
    "* Let's assume there are 150 samples, including 50 salmon, 50 sea bass and 50 cod.  Suppose our model misclassifies 3 salmon, 2 sea bass and 4 cod.\n",
    "\n",
    "* Prediction accuracy of our binary classification model is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{accuracy} = \\frac{47+48+46}{50+50+50} = \\frac{47}{50}\n",
    "\\end{equation}\n",
    "\n",
    "* Prediction error is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{error} = \\frac{N_{mis}}{N} = \\frac{3+2+4}{50+50+50} = \\frac{3}{50}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "* A confusion matrix summarizes the classification accuracy across several classes. It shows the ways in which our classification model is confused when it makes predictions, allowing visualization of the performance of our algorithm. Generally, each row represents the instances of an actual class while each column represents the instances in a predicted class. \n",
    "#### Binary classification example\n",
    "\n",
    "* In case of binary classifier the prediction will be one of 2 classes, for instance let the 2 classes be P and N. The figure below shows the prediction results in a confusion matrix:\n",
    "<img src=\"figures/confusion_matrix_example.png\"  style=\"width: 300px;\"/>\n",
    "\n",
    "* True positive (TP): correctly predicting event values, number of predictions where the classifier predicts P as P.\n",
    "* False positive (FP): incorrectly calling non-events as an event, number of predictions where the classifier predicts N as P.\n",
    "* True negative (TN): correctly predicting non-event values, number of predictions where the classifier predicts N as N.\n",
    "* False negative (FN): incorrectly labeling events as non-event, number of predictions where the classifier predicts P as N.\n",
    "\n",
    "#### Multi-class classification example\n",
    "* If our classifier is trained to distinguish between salmon, sea bass and cod, then we can summarize the prediction result in the confusion matrix as follows:\n",
    "    \n",
    "\n",
    "| Actual/Predicted | Salmon | Sea bass | Cod  |\n",
    "| --- | --- | --- | --- |\n",
    "| Salmon | 47 | 2 | 1 |\n",
    "| Sea Bass | 2 | 48 | 0 |\n",
    "| Cod | 0 | 0 | 50 |\n",
    "\n",
    "* In this confusion matrix, of the 50 actual salmon, the classifier predicted that 2 are sea bass, 1 is cod incorrectly and 47 are labeled salmon correctly. All correct predictions are located in the diagonal of the table. So it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal. \n",
    "\n",
    "\n",
    "#### Common Performance Measures\n",
    "* Precision is also called positive predictive value.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}\n",
    "\\end{equation}\n",
    "\n",
    "* Recall is also called true positive rate, probability of detection\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "* Fall-out is also called false positive rate, probability of false alarm.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Fall-out} = \\frac{\\text{FP}}{\\text{All negative samples}}= \\frac{\\text{FP}}{\\text{FP}+\\text{TN}}\n",
    "\\end{equation}\n",
    "\n",
    "* *Consider the salmon/non-salmon classification problem, what are the TP, FP, TN, FN values?*\n",
    "\n",
    "| Actual/Predicted | Salmon | Non-Salmon  |\n",
    "| --- | --- | --- | \n",
    "| Salmon | 47 | 3 | \n",
    "| Non-Salmon | 2 | 98 | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves \n",
    "\n",
    "* The Receiver Operating Characteristic (ROC) curve is a plot between the true positive rate (TPR) and the false positive rate (FPR), where the TPR is defined on the $y$-axis and FPR is defined on the $x$-axis. \n",
    "\n",
    "* $TPR = TP/(TP+FN)$ is defined as ratio between true positive prediction and all real positive samples. The definition used for $FPR$ in a ROC curve is often problem dependent.  For example, for detection of targets in an area, FPR may be defined as the ratio between the number of false alarms per unit area ($FA/m^2$).  In another example, if you have a set number of images and you are looking for targets in these collection of images, FPR may be defined as the number of false alarms per image.  In some cases, it may make the most sense to simply use the Fall-out or false positive rate.\n",
    "\n",
    "* Given a binary classifier and its threshold, the (x,y) coordinates of ROC space can be calculated from all the prediction result.  You trace out a ROC curve by varying the threshold to get all of the points on the ROC.\n",
    "\n",
    "* The diagonal between (0,0) and (1,1) separates the ROC space into two areas, which are left up area and right bottom area. The points above the diagonal represent good classification (better than random guess) which below the diagonal represent bad classification (worse than random guess).\n",
    "\n",
    "* *What is the perfect prediction point in a ROC curve?*\n",
    "\n",
    "### Precision-Recall curves\n",
    "\n",
    "* ROC curves trace out the TPR vs. FPR over many thresholds.  \n",
    "\n",
    "* Similarly, other metrics can be plotted over many thresholds.  Another common example is Precision-Recall curves\n",
    "\n",
    "* PR curves are generated the same way as ROC curves, however, instead of plotting TPR vs. FPR, Precision vs. Recall (as defined above) are plotted over many thresholds.\n",
    "\n",
    "* *What does the perfect PR curve look like?*\n",
    "\n",
    "* PR curves are often preferred over ROC curves in cases of severely imbalanced data.  \n",
    "\n",
    "* Similar to ROC and PR curves, any statistics that can be computed via the confusion matrix, can be plotted over all possible thresholds.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE and MAE\n",
    "\n",
    "\n",
    "* *Mean Square Error* (MSE) is the average of the squared error between prediction and actual observation. \n",
    "\n",
    "* For each sample $\\mathbf{x}_i$, the prediction value is $y_i$ and the actual output is $d_i$. The MSE is\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\sum_{i=1}^n \\frac{(d_i - y_i)^2}{n}\n",
    "\\end{equation}\n",
    "\n",
    "* *Root Mean Square Error* (RMSE) is simply the square root the MSE. \n",
    "\n",
    "\\begin{equation}\n",
    "RMSE = \\sqrt{MSE}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "* *Mean Absolute Error* (MAE) is the average of the absolute error.\n",
    "\\begin{equation}\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^n \\lvert d_i - y_i \\rvert\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Final Thoughts/Summary\n",
    "\n",
    "Of course, there are many other evaluation metrics.  These are just a few of the most commonly used.  In practice, the *best* evaluation metrics are those that provide you insight into your problem - particularly when it is not tied closely to your objective function.\n",
    "\n",
    "<img src=\"figures/evaluation_meme.png\"  style=\"width: 500px;\"/>\n",
    "\n",
    "-F. Diaz, Source: https://twitter.com/841io/status/1405184102798667777?s=20\n",
    "\n",
    "It is important to remember Goodhart's Law when using evaluation metrics (https://en.wikipedia.org/wiki/Goodhart%27s_law): \n",
    "\n",
    "\"When a measure becomes a target, it ceases to be a good measure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
