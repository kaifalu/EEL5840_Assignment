{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Design in Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How can we estimate the expected performance of a machine learning algorithm for a particular application? \n",
    "\n",
    "* How do we select between machine learning algorithms (or parameter settings)? \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Commonly, we split a data set into three groups: training, validation and test. (Only for evaluation purposes - often use ALL data for a final training of the system) \n",
    "* The training and validation sets may be rotated (e.g., using cross-validation)\n",
    "* As previously discussed, we cannot rely on training (or validation) performance (because of the potential for over-fitting) to help us answer the questions above\n",
    "* Our performance estimate is only as good as our test set is representative of the true test data in application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We also cannot rely on one training run of the algorithm:\n",
    "  - variations in training/validation sets\n",
    "  - random factors during training (e.g., random initialization, local optima, etc.)\n",
    "        \n",
    "* The No Free Lunch Theorem: There is no universally best algorithm. For any machine learning algorithm there are sets of data where it works well and sets where it works poorly\n",
    "\n",
    "* Performance of an algorithm can be determined using any of the measures we discussed previously (e.g., error rate, accuracy, ROC curves, Perf-Recall curves, etc) but also in terms of:\n",
    "    - Risk\n",
    "    - Running time\n",
    "    - Training time\n",
    "    - Run time storage/memory\n",
    "    - Train time storage/memory\n",
    "    - Computational complexity\n",
    "    - Interpretability\n",
    "    - etc...\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The output of a trained learning system depends on:\n",
    "    - Controllable parameters: hyperparameters/settings of the algorithm/algorithm design choices\n",
    "    - Uncontrollable parameters: noise in data, any randomness in training \n",
    "        \n",
    "* To fully test a system, you want to try to evaluate each of these parameters separately.  However, this is often not easily done.\n",
    "* Various strategies:\n",
    "    - Best guess\n",
    "    - Varying one factor at a time\n",
    "    - Full/Partial Factorial design\n",
    "\n",
    "* If there is randomization in your experiment, need to run multiple times (replicate experiments) to estimate the expected result (e.g., mean and variance of performance)\n",
    "* If there are real-world impacts to your runs (e.g., machine warming up, time of day, etc), need to randomize your trials across factors\n",
    "* Need to ensure we are comparing the parameters and algorithms we are interested and not any confounding factors (e.g., if you want to compare one parameter  - ensure everything else is fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When conducting experiments:\n",
    "    - Understand the goal of your study\n",
    "    - Determine your evaluation measure(s)\n",
    "    - Determine what factors to vary and how to vary them\n",
    "    - Design your experiment (and get estimate of how long it will take using a couple trial subset runs)\n",
    "    - Perform the experiment\n",
    "    - Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation and Training, Validation, Testing Data Sets\n",
    "\n",
    "* Since we can (often, easily) overfit, our error or prediction performance on a training data set is not a good indication of performance on unknown test data.  \n",
    "* One way estimate test performance of a system on unknown test data is to use some of the training data for training and some for validation (to act like unknown test data). \n",
    "* If you are repeatedly changing your model/adjusting parameters/tweaking your algorithm, you may even over fit the hold-out validation set.  So, you can hold out yet another set for testing. \n",
    "* However, in general, we only have a limited amount of training data. So, we want to use as much of it as possible for training.  One strategy to balance the tradeoff between needing training data and validation data is to use cross-validation.\n",
    "* Cross-validation can also give an indication of stability/robustness of your method. \n",
    "* However there are downsides to cross-validation: need to train many times (which can sometimes be very computationally complex.), and you end up with several models - how do you pick the final one to use?\n",
    " \n",
    "* For further reading and reference: Simon Haykin. Neural Networks A Comprehensive Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
