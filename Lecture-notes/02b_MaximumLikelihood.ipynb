{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* *Method of Maximum Likelihood:*\n",
    "    * A *data likelihood* is how likely the data is given the parameter set\n",
    "    * So, if we want to maximize how likely the data is to have come from the model we fit, we should find the parameters that maximize the likelihood\n",
    "    * A common trick to maximizing the likelihood is to maximize the log likelihood.  Often makes the math much easier.  *Why can we maximize the log likelihood instead of the likelihood and still get the same answer?*\n",
    "    * Consider: $\\max \\ln \\exp \\left\\{ -\\frac{1}{2}\\left(y(x_n, \\mathbf{w}) - t_n\\right)^2\\right\\}$ We go back to our original objective. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood for the Bernoulli Distribution\n",
    "\n",
    "* Lets look at this in terms of binary variables, e.g., Flipping a coin:  $X =1$ is heads, $X=0$ is tails\n",
    "* Let $\\mu$ be the probability of heads.  If we know $\\mu$, then: $P(x = 1 |\\mu) = \\mu$ and $P(x = 0|\\mu) = 1-\\mu$\n",
    "\\begin{eqnarray}\n",
    "P(x|\\mu) = \\mu^x(1-\\mu)^{1-x} = \\left\\{\\begin{array}{c c}\\mu & \\text{ if } x=1 \\\\ 1-\\mu & \\text{ if } x = 0 \\end{array}\\right.\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is called the *Bernoulli* distribution.  The mean and variance of a Bernoulli distribution is: \n",
    "\\begin{equation}\n",
    "E[x] = \\mu\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "E\\left[(x-\\mu)^2\\right] = \\mu(1-\\mu)\n",
    "\\end{equation}\n",
    "* So, suppose we conducted many Bernoulli trials (e.g., coin flips) and we want to estimate $\\mu$\n",
    "\n",
    "### Method: Maximum Likelihood\n",
    "\\begin{eqnarray}\n",
    "p(\\mathscr{D}|\\mu) &=& \\prod_{n=1}^N p(x_n|\\mu) \\\\\n",
    "&=& \\prod_{n=1}^N \\mu^{x_n}(1-\\mu)^{1-x_n}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Maximize : (*What trick should we use?*)\n",
    "\\begin{eqnarray}\n",
    "\\mathscr{L} = \\sum_{n=1}^N x_n \\ln \\mu + (1-x_n)\\ln(1-\\mu)\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\mathscr{L}}{\\partial \\mu} =  0 &=& \\frac{1}{\\mu}\\sum_{n=1}^N x_n - \\frac{1}{1-\\mu }\\sum_{n=1}^N (1 - x_n)\\\\\n",
    "0 &=& \\frac{(1-\\mu) \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N (1- x_n)}{\\mu(1-\\mu)}\\\\\n",
    "0 &=& \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N x_n - \\mu \\sum_{n=1}^N 1 + \\mu \\sum_{n=1}^N x_n\\\\\n",
    "0 &=& \\sum_{n=1}^N x_n - \\mu N\\\\\n",
    "\\mu &=& \\frac{1}{N}\\sum_{n=1}^N x_n = \\frac{m}{N}\n",
    "\\end{eqnarray}\n",
    "where $m$ is the number of successful trials. \n",
    "\n",
    "* So, if we flip a coin 1 time and get heads, then $\\mu = 1$ and probability of getting tails is 0.  *Would you believe that? We need a prior!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Distribution:\n",
    "* Consider a univariate Gaussian distribution:\n",
    "\\begin{equation}\n",
    "\\mathscr{N}(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2} \\right\\}\n",
    "\\end{equation}\n",
    "* $\\sigma^2$ is the variance OR $\\frac{1}{\\sigma^2}$ is the *precision*\n",
    "* So, as $\\lambda$ gets big, variance gets smaller/tighter.  As $\\lambda$ gets small, variance gets larger/wider.\n",
    "* The Gaussian distribution is also called the *Normal* distribution. \n",
    "* We will often write $N(x|\\mu, \\sigma^2)$ to refer to a Gaussian with mean $\\mu$ and variance $\\sigma^2$.\n",
    "* *What is the multi-variate Gaussian distribution?* \n",
    "\n",
    "* What is the expected value of $x$ for the Gaussian distribution?\n",
    "\\begin{eqnarray}\n",
    "E[x] &=& \\int x p(x) dx \\\\\n",
    "     &=& \\int x \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left\\{ -\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2} \\right\\} dx\n",
    "\\end{eqnarray}\n",
    "* *Change of variables:*  Let\n",
    "\\begin{eqnarray}\n",
    "y &=& \\frac{x-\\mu}{\\sigma} \\rightarrow x = \\sigma y + \\mu\\\\\n",
    "dy &=& \\frac{1}{\\sigma} dx \\rightarrow dx = \\sigma dy\n",
    "\\end{eqnarray}\n",
    "* Plugging this into the expectation: \n",
    "\\begin{eqnarray}\n",
    "E[x] &=& \\int \\left(\\sigma y + \\mu  \\right)\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} \\sigma dy \\\\\n",
    "&=& \\int \\frac{\\sigma y}{\\sqrt{2\\pi}} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} dy + \\int \\frac{\\mu}{\\sqrt{2\\pi}} \\exp\\left\\{ - \\frac{1}{2} y^2 \\right\\} dy \n",
    "\\end{eqnarray}\n",
    "* The first term is an odd function: $f(-y) = -f(y)$  So, $E[x] = 0 + \\mu = \\mu$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MLE of Mean of Gaussian\n",
    "\n",
    "*  Let $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_N$ be samples from a multi-variance Normal distribution with known covariance matrix and an unknown mean.  Given this data, obtain the ML estimate of the mean vector. \n",
    "\t\\begin{equation}\n",
    "\tp(\\mathbf{x}_k| {{\\mu}}) = \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_k - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_k - {\\mu})\\right)\n",
    "\t\\end{equation}\n",
    "* We can define our likelihood given the $N$ data points.  We are assuming these data points are drawn independently but from an identical distribution (i.i.d.):\n",
    "\t\\begin{equation}\n",
    "\t\\prod_{n=1}^N p(\\mathbf{x}_n| {{\\mu}}) = \\prod_{n=1}^N \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\n",
    "\t\\end{equation}\n",
    "*  We can apply our \"trick\" to simplify\n",
    "\t\\begin{eqnarray}\n",
    "\t\\mathscr{L} &=& \\ln \\prod_{n=1}^N p(\\mathbf{x}_n| {{\\mu}}) = \\ln \\prod_{n=1}^N \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\\\\\n",
    "\t&=& \\sum_{n=1}^N  \\ln \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}}\\exp\\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right)\\\\\n",
    "\t&=& \\sum_{n=1}^N  \\left( \\ln \\frac{1}{(2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}}} + \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu})\\right) \\right) \\\\\n",
    "\t&=&  - N \\ln (2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}} + \\sum_{n=1}^N  \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) \\right) \n",
    "\t\\end{eqnarray}\n",
    "* Now, lets maximize:\n",
    "\t\\begin{eqnarray}\n",
    "\t\\frac{\\partial \\mathscr{L}}{\\partial \\mu} &=& \\frac{\\partial}{\\partial \\mu} \\left[- N \\ln (2\\pi)^{\\frac{l}{2}}\\left| \\Sigma \\right|^{\\frac{1}{2}} + \\sum_{n=1}^N  \\left( -\\frac{1}{2}(\\mathbf{x}_n - {{\\mu}})^T\\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) \\right)\\right] = 0 \\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N  \\Sigma^{-1}(\\mathbf{x}_n - {\\mu}) = 0\\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N  \\Sigma^{-1}\\mathbf{x}_n  = \\sum_{n=1}^N  \\Sigma^{-1} {\\mu}\\\\\n",
    "\t&\\rightarrow& \\Sigma^{-1} \\sum_{n=1}^N \\mathbf{x}_n  = \\Sigma^{-1} {\\mu} N\\\\\n",
    "\t&\\rightarrow& \\sum_{n=1}^N \\mathbf{x}_n  = {\\mu} N\\\\\n",
    "\t&\\rightarrow& \\frac{\\sum_{n=1}^N \\mathbf{x}_n}{N} = {\\mu}\\\\\n",
    "\t\\end{eqnarray}\n",
    "* So, the ML estimate of $\\mu$ is the sample mean!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
