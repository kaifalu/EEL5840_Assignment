{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis - Maximal Variance Formulation\n",
    "\n",
    "* PCA is a linear transformation\n",
    "* PCA minimizes the redundancy of the resulting transformed data (by ending up data that is uncorrelated), minimizes the mean squared error between original and transformed/reduced data, and maximizes the retained variance of the data. \n",
    "\n",
    "* Consider a data set of observations $\\left\\{ \\mathbf{x}_n \\right\\}_{n=1}^N$ and $\\mathbf{x}_n \\in \\mathbb{R}^D$. We want to maximize the variance of the projected data. \n",
    "\n",
    "* Let us first consider reducing dimensionality to $M = 1$.  Let us define the projection as a vector $\\mathbf{u}_1$ where $\\mathbf{u}_1^T\\mathbf{u}_1 = 1$.  Then, each projected data point into 1-D would be $y_n = \\mathbf{u}_1^T\\mathbf{x}_n$\n",
    "\n",
    "* The mean of the sample data is $\\bar{\\mathbf{x}} = \\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n$ and the mean of the projected data is $\\mathbf{u}_1^T\\bar{\\mathbf{x}}$\n",
    "\n",
    "* The variance of projected data is: \n",
    "\\begin{eqnarray}\n",
    "\\frac{1}{N} \\sum_{n=1}^N \\left\\{ \\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\bar{\\mathbf{x}} \\right\\}^2 & = & \\frac{1}{N} \\sum_{n=1}^N  \\left( \\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\bar{\\mathbf{x}} \\right)\\left( \\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\bar{\\mathbf{x}} \\right)^T\\\\\n",
    "& = & \\frac{1}{N} \\sum_{n=1}^N  \\left( \\mathbf{u}_1^T\\mathbf{x}_n - \\mathbf{u}_1^T\\bar{\\mathbf{x}} \\right) \\left( \\mathbf{x}_n^T\\mathbf{u}_1 - \\bar{\\mathbf{x}}^T\\mathbf{u}_1 \\right)\\\\\n",
    "& = & \\frac{1}{N} \\sum_{n=1}^N  \\mathbf{u}_1^T\\mathbf{x}_n\\mathbf{x}_n^T\\mathbf{u}_1 - \\mathbf{u}_1^T\\mathbf{x}_n\\bar{\\mathbf{x}}^T\\mathbf{u}_1 - \\mathbf{u}_1^T\\bar{\\mathbf{x}}\\mathbf{x}_n^T\\mathbf{u}_1 + \\mathbf{u}_1^T\\bar{\\mathbf{x}}\\bar{\\mathbf{x}}^T\\mathbf{u}_1\\\\\n",
    "& = & \\mathbf{u}_1^T \\left( \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n\\mathbf{x}_n^T - \\mathbf{x}_n\\bar{\\mathbf{x}}^T - \\bar{\\mathbf{x}}\\mathbf{x}_n^T+ \\bar{\\mathbf{x}}\\bar{\\mathbf{x}}^T\\right)\\mathbf{u}_1\\\\\n",
    "& = & \\mathbf{u}_1^T  \\left( \\frac{1}{N} \\sum_{n=1}^N  \\left(\\mathbf{x}_n - \\bar{\\mathbf{x}}\\right)\\left(\\mathbf{x}_n - \\bar{\\mathbf{x}}\\right)^T \\right) \\mathbf{u}_1\\\\\n",
    "& = & \\mathbf{u}_1^T\\mathbf{S}\\mathbf{u_1}\n",
    "\\end{eqnarray}\n",
    "\n",
    "* Now, we can maximize the projected variance with respect to $\\mathbf{u}_1$ while constraining $\\mathbf{u}_1^T\\mathbf{u}_1 = 1$.  We will do this using a Lagrange multiplier: \n",
    "\\begin{equation}\n",
    "L = \\mathbf{u}_1^T\\mathbf{S}\\mathbf{u_1} + \\lambda_1\\left(1 - \\mathbf{u}_1^T\\mathbf{u}_1\\right)\n",
    "\\end{equation}\n",
    "\n",
    "* By taking the derivative of the Lagrangian and setting it equal to zero, we get: \n",
    "\\begin{equation}\n",
    "\\mathbf{S}\\mathbf{u}_1 = \\lambda_1\\mathbf{u_1}\n",
    "\\end{equation}\n",
    "\n",
    "* We can multiply the left side by $\\mathbf{u}_1^T$ and get:\n",
    "\t\\begin{equation}\n",
    "\t\\mathbf{u_1}^T\\mathbf{S}\\mathbf{u_1} = \\lambda_1\n",
    "\t\\end{equation}\n",
    "\n",
    "* So the variance of the projected data is equal to the eigenvalue of the covariance matrix of the sample data along the direction of the eigenvector used for dimensionality reduction. \n",
    "* We can incrementally add new eigenvector directions (ordered by maximal eigenvalue/variance) to project into an $M$ dimensional space where $1\\leq M \\leq D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Minimization of Mean Squared Error\n",
    "\n",
    "* We can also look at PCA as a minimization of mean squared error.  \n",
    "* Consider $\\mathbf{x}\\in R^n$ and an orthogonal basis $\\mathbf{a}$:\n",
    "\n",
    " \\begin{equation}\n",
    " \\hat{\\mathbf{x}} = \\sum_{i=1}^m y_i\\mathbf{a}_i\n",
    " \\end{equation}\n",
    " where $m < n$.  \n",
    " \\begin{equation}\n",
    " y_j = \\mathbf{x}^T\\mathbf{a}_j\n",
    " \\end{equation} where $\\mathbf{A}^T\\mathbf{A}=\\mathbf{I}$\n",
    "\n",
    "We want to minimize the residual error: \n",
    "\\begin{equation}\n",
    "\\epsilon = \\mathbf{x} - \\hat{\\mathbf{x}} = \\sum_{j=m+1}^n y_j \\mathbf{a}_j\n",
    "\\end{equation}\n",
    "The objective we will use is the mean square residual:\n",
    "\\begin{eqnarray}\n",
    "J &=& E\\{ \\|\\epsilon\\|^2_2\\}\\\\\n",
    "&=& E\\left\\{\\left( \\sum_{i=m+1}^n y_i \\mathbf{a}_i^T\\right)\\left( \\sum_{i=m+1}^n y_i \\mathbf{a}_i\\right) \\right\\}\\\\\n",
    "&=&\\sum_{j=m+1}^n E \\{y_j^2\\}\\\\\n",
    "&=&\\sum_{j=m+1}^n E \\{(\\mathbf{a}_j^T\\mathbf{x})(\\mathbf{x}^T\\mathbf{a}_j)\\}\\\\\n",
    "&=& \\sum_{j=m+1}^n \\mathbf{a}_j^T E\\{\\mathbf{x}\\mathbf{x}^T\\}\\mathbf{a}_j\\\\\n",
    "&=& \\sum_{j=m+1}^n \\mathbf{a}_j^T R_x\\mathbf{a}_j\n",
    "\\end{eqnarray}\n",
    "Minimize the error and incorporate Lagrange parameters for $\\mathbf{A}^T\\mathbf{A}=\\mathbf{I}$:\n",
    "\\begin{equation}\n",
    "L = \\sum_{j=m+1}^n \\mathbf{a}_j^T R_x\\mathbf{a}_j - \\sum_{j=m+1}^n \\lambda_J \\mathbf{a}_j^T\\mathbf{a}_j -1\n",
    "\\end{equation}\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial J}{\\partial \\mathbf{a}_j} &=& 2(R_x\\mathbf{a}_j - \\lambda_j\\mathbf{a}_j) = 0 \\text{ for }j = m+1 \\ldots n\\\\\n",
    "R_x\\mathbf{a}_j &=& \\lambda_j\\mathbf{a}_j\n",
    "\\end{eqnarray}\n",
    "So, the sum of the error is the sum of the eigenvalues of the unused eigenvectors.  So, we want to select the eigenvectors with the $m$ largest values. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
